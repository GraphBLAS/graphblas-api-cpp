# GraphBLAS Languages Committee Meeting (on C++ API) - June 7, 2022

## Attendees
- [ ] Ben Brock
- [X] Scott McMillan
- [X] Tim Mattson
- [ ] Aydin Buluc
- [X] Jose Moreira

## Agenda
Scott spent the bulk of the meeting explaining my thoughts surrounding the Semiring concept in the context of mxm.


## Minutes

I spent the bulk of the meeting explaining my thoughts surrounding the Semiring concept in the context of mxm (because it does not make sense to me to discuss the issue of semiring anywhere except in the context of matrix multiplication…sorry I have a mental block otherwise).  At some point Jose asked a really good question: “If I use this interface will the result be deterministic?” and I don’t know how to get at the answer at the moment but I think we should consider the following to work through it (paper being presented at ECOOP 2022 this month):

The first (and major) phase of the generic programming process is sometimes known as “lifting” where we create generic algorithms through a process of successive generalization.
That is, the process is
1.	Study the concrete implementation of an algorithm; (We already have some)
2.	Lift away unnecessary requirements to produce a more abstract algorithm; (I am in this process now)
3.	Repeat the lifting process until we have obtained a generic algorithm that is as general as possible but that still instantiates to efficient concrete implementations; and
4.	Catalog remaining requirements and organize them into concepts.

Once the algorithm (like mxm) is lifted and made appropriately generic it is specialized (the dual of lifting) through composition of concrete types to realize a concrete implementation.  A single algorithm can be specialized in many different ways depending on the concrete types and I believe it is our job to allow for the widest range of valid implementations.  But we still need to decide what the valid implementations are and this is related to what Jose was trying to address last week…I think that one mxm (or multiply) function signature can cover the two philosophies.

I need to start at the last slide I showed.

```
template <ConstMatrixView AMatrixType,
          ConstMatrixView BMatrixType,
          BinaryOp        ReduceType = std::plus<>,  //All three types the same?
          BinaryOp        CombineType = std::multiplies<>,
          MaskMatrixView  MaskType = grb::full_mask<>
          > //requires the types relationship???
auto multiply(AMatrixType &&a,
              BMatrixType &&b,
              ReduceType  &&reduce  = ReduceType(),
              CombineType &&combine = CombineType(),
              MaskType    &&mask    = MaskType()); // or grb::full_mask());
```

I believe the question we should be asking: what is the minimum required by mxm (or multiply).  

- Since it is computing a sparse result we don’t strictly need to know what the reducer’s identity is
  -	But if there was an identity then an implementation could use a different implementation (could be useful for multithreaded or distributed or Joe Eaton implementations)
- What if we knew the reducer’s annihilator?
  -	The implementation could potentially short circuit the reduction and exit early 
- What if we knew the reducer is associative
  -	The implementation could more than just in-order reductions and be able to implement others that use divide and conquer
- What if we knew the reducer is commutative
  -	The implementation would be free to reorder and distribute the computation in any way it wants (e.g. cyclic distributions).

Getting back to Jose’s question,  what is actually required to ensure implementations are deterministic.  Is it that the input types and the output type of the reducer need to be all one type?
